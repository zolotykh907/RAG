services:
  indexing:
    build:
      context: .
      dockerfile: ./docker/indexing.Dockerfile
    container_name: indexing
    volumes:
      - model_cache:/root/.cache/huggingface/hub
      - shared_data:/app/data

    environment:
      - PYTHONUNBUFFERED=1

  ollama:
    image: ollama/ollama
    container_name: ollama
    volumes:
      - ollama:/root/.ollama
    ports:
      - "11434:11434"
    entrypoint: ["/bin/bash", "-c", "ollama serve & sleep 2 && ollama pull llama3 && wait"]

  query:
      build: 
        context: .
        dockerfile: ./docker/query.Dockerfile
      container_name: query
      volumes:
        - shared_data:/app/data
        - model_cache:/root/.cache/huggingface/hub
      ports:
        - "8000:8000"
      depends_on:
        - ollama
      environment:
        - OLLAMA_HOST=http://ollama:11434

  backend:
    build: 
      context: .
      dockerfile: ./docker/backend.Dockerfile
    container_name: backend
    volumes:
      - shared_data:/app/data
    
  frontend:
    build:
      context: .
      dockerfile: ./docker/frontend.Dockerfile
    container_name: frontend
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app/frontend
      - /app/frontend/node_modules
    working_dir: /app/frontend
    environment:
      - CHOKIDAR_USEPOLLING=true
    command: npm start


volumes:
  ollama:
  shared_data:
  model_cache: