services:
  indexing:
    build:
      context: .
      dockerfile: ./docker/indexing.Dockerfile
    container_name: indexing
    volumes:
      - model_cache:/root/.cache/huggingface/hub
      - shared_data:/app/data

    environment:
      - PYTHONUNBUFFERED=1

  ollama:
    image: ollama/ollama
    container_name: ollama
    volumes:
      - ollama:/root/.ollama
    ports:
      - "11434:11434"
    entrypoint: ["/bin/bash", "-c", "ollama serve & sleep 2 && ollama pull llama3 && wait"]

  query:
      build: 
        context: .
        dockerfile: ./docker/query.Dockerfile
      container_name: query
      volumes:
        - shared_data:/app/data
        - model_cache:/root/.cache/huggingface/hub
      ports:
        - "8000:8000"
      depends_on:
        - ollama
      environment:
        - OLLAMA_HOST=http://ollama:11434

volumes:
  ollama:
  shared_data:
  model_cache: